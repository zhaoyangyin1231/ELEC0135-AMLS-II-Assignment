{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "taskB_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UOxqzUqBbxnM"
      },
      "outputs": [],
      "source": [
        "# subtask B in both English and Arabic\n",
        "\n",
        "# import\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "import gensim\n",
        "import re\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('DataCleanB_Eng.csv')  # English Data\n",
        "# df = pd.read_csv('DataCleanB_Arabic.csv')  # Arabi Data\n",
        "\n",
        "# split data\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "metadata": {
        "id": "8S5jHD-mRtLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31919d6-9582-4471-e4a8-51e78263bb29"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN size: 3447\n",
            "TEST size: 862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wotd2vec\n",
        "def word_to_vector(df):\n",
        "  doc_text = [_text.split() for _text in df.Text] \n",
        "  doc_topic = [_text.split() for _text in df.Topic] \n",
        "  docs = doc_text + doc_topic\n",
        "  # docs = [_text.split() for _text in df.Text] \n",
        "  w2v_model = gensim.models.word2vec.Word2Vec(size=300, window=7, min_count=10, workers=8)\n",
        "  w2v_model.build_vocab(docs)\n",
        "  words = w2v_model.wv.vocab.keys()\n",
        "  vocab_size = len(words)\n",
        "  w2v_model.train(docs, total_examples=len(docs), epochs=8)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(df.Text)\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  return tokenizer, vocab_size, w2v_model"
      ],
      "metadata": {
        "id": "4Nz9-tIkGJjS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare train test data\n",
        "def data_prepare(tokenizer):\n",
        "  x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.tidy), maxlen=300)\n",
        "  x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.tidy), maxlen=300)\n",
        "  labels = df_train.Label.unique().tolist()\n",
        "  # print(x_train)\n",
        "  # print(x_test)\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(df_train.Label.tolist())\n",
        "\n",
        "  y_train = encoder.transform(df_train.Label.tolist())\n",
        "  y_test = encoder.transform(df_test.Label.tolist())\n",
        "\n",
        "  y_train = y_train.reshape(-1,1)\n",
        "  y_test = y_test.reshape(-1,1)\n",
        "  # print(y_train.shape)\n",
        "  # print(x_train.shape)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "def embedding_layer(vocab_size, w2v_model):\n",
        "  # embedding layer\n",
        "  embedding_matrix = np.zeros((vocab_size, 300))\n",
        "  # print(embedding_matrix.shape)\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "      embedding_matrix[i] = w2v_model.wv[word]\n",
        "  # print(embedding_matrix.shape)\n",
        "\n",
        "  embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)\n",
        "  # embedding_layer = Embedding(vocab_size, 300, input_length=300)\n",
        "\n",
        "  return embedding_layer\n",
        "\n",
        "def build_lstm_model(embedding_layer):\n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  # model.add(embedding_layer)\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(LSTM(1024, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_model(model, x_train, x_test, y_train, y_test):\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])\n",
        "  callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n",
        "  \n",
        "  y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
        "  y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
        "  print(y_train.shape)\n",
        "  print(x_train.shape)\n",
        "\n",
        "  y_train = np.array(y_train)\n",
        "  X_train = np.array(x_train)\n",
        "  y_test = np.array(y_test)\n",
        "  X_test = np.array(x_test)\n",
        "  print(y_train.shape)\n",
        "  print(x_train.shape)\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "                      batch_size=128,\n",
        "                      epochs=10,\n",
        "                      validation_split=0.1,\n",
        "                      verbose=1,\n",
        "                      callbacks=callbacks)\n",
        "  "
      ],
      "metadata": {
        "id": "vnRc9RpaFmnG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "tokenizer, vocab_size, w2v_model = word_to_vector(df)\n",
        "x_train, x_test, y_train, y_test = data_prepare(tokenizer)\n",
        "embedding_layer = embedding_layer(vocab_size, w2v_model)\n",
        "model = build_lstm_model(embedding_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwHagUSyF-pu",
        "outputId": "5e1619ad-f1fe-49e1-8432-723c435678d7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 300, 300)          3812100   \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 300, 300)          0         \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 1024)              5427200   \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,765,639\n",
            "Trainable params: 5,953,539\n",
            "Non-trainable params: 3,812,100\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train lstm model\n",
        "train_model(model, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LP5p1RnfxX7",
        "outputId": "ccc96816-c80a-4f5c-8f39-7718061c7589"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3447, 3)\n",
            "(3447, 300)\n",
            "(3447, 3)\n",
            "(3447, 300)\n",
            "Epoch 1/10\n",
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final test accuracy\n",
        "score = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkSbIeQZiCkb",
        "outputId": "254f723d-0bbc-4d87-c6eb-08b4801ae9bc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 9s 1s/step - loss: 1.0594 - accuracy: 0.8213\n",
            "\n",
            "ACCURACY: 0.8213456869125366\n",
            "LOSS: 1.0593801736831665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cYvLonUmWBH3"
      }
    }
  ]
}