{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "taskA_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UOxqzUqBbxnM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import gensim\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "df = pd.read_csv('twitterA_train_data.txt', sep=\"\\t\",names=['Number','Label','Text'])\n",
        "\n",
        "# split data\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))\n"
      ],
      "metadata": {
        "id": "8S5jHD-mRtLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adda89ea-2139-4725-f92c-f0610d17ed1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN size: 12441\n",
            "TEST size: 3111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wotd2vec\n",
        "def word_to_vector(df):\n",
        "  docs = [_text.split() for _text in df.Text] \n",
        "  w2v_model = gensim.models.word2vec.Word2Vec(size=300, window=7, min_count=10, workers=8)\n",
        "  w2v_model.build_vocab(docs)\n",
        "  words = w2v_model.wv.vocab.keys()\n",
        "  vocab_size = len(words)\n",
        "  w2v_model.train(docs, total_examples=len(docs), epochs=8)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(df.Text)\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "4Nz9-tIkGJjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare train test data\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.tidy), maxlen=300)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.tidy), maxlen=300)\n",
        "labels = df_train.Label.unique().tolist()\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_train.Label.tolist())\n",
        "\n",
        "y_train = encoder.transform(df_train.Label.tolist())\n",
        "y_test = encoder.transform(df_test.Label.tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "def embedding_layer():\n",
        "  # embedding layer\n",
        "  embedding_matrix = np.zeros((vocab_size, 300))\n",
        "  print(embedding_matrix.shape)\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "      embedding_matrix[i] = w2v_model.wv[word]\n",
        "  print(embedding_matrix.shape)\n",
        "\n",
        "  embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)\n",
        "  # embedding_layer = Embedding(vocab_size, 300, input_length=300)\n",
        "\n",
        "  return embedding_layer\n",
        "\n",
        "def build_lstm_model(embedding_layer):\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  model.add(Dropout(0.5))\n",
        "  # model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Bidirectional(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), input_shape=(300, 300)))\n",
        "  model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2)))\n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_model(model, x_train, x_test, y_train, y_test):\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])\n",
        "  callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n",
        "  \n",
        "  y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
        "  y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
        "\n",
        "  y_train = np.array(y_train)\n",
        "  X_train = np.array(x_train)\n",
        "  y_test = np.array(y_test)\n",
        "  X_test = np.array(x_test)\n",
        "\n",
        "  # y_train = utils.to_categorical(y_train, num_classes=3)\n",
        "  # y_test = utils.to_categorical(y_test, num_classes=3)\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "                      batch_size=128,\n",
        "                      epochs=20,\n",
        "                      validation_split=0.1,\n",
        "                      verbose=1,\n",
        "                      callbacks=callbacks)\n",
        "  "
      ],
      "metadata": {
        "id": "vnRc9RpaFmnG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "tokenizer = word_to_vector(df)\n",
        "embedding_layer = embedding_layer()\n",
        "model = build_lstm_model(embedding_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwHagUSyF-pu",
        "outputId": "ded628ce-6ede-421b-a4fc-ce1224db9a17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 300)          10177500  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 300, 300)          0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 300, 512)         1140736   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 512)              1574912   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              525312    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 1024)             4096      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 3075      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,425,631\n",
            "Trainable params: 13,423,583\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train lstm model\n",
        "train_model(model, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqSCfLZ-BsNi",
        "outputId": "cff10b7c-cc30-46ad-c7f3-9c78ffa91f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final test accuracy\n",
        "score = model.evaluate(x_test, y_test, batch_size=800)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-DqmhyYCX7T",
        "outputId": "1299725d-66a6-4cab-caa8-9d2a784b73d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 23s 6s/step - loss: 1.2511 - accuracy: 0.6127\n",
            "\n",
            "ACCURACY: 0.6126647591590881\n",
            "LOSS: 1.2511096000671387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cYvLonUmWBH3"
      }
    }
  ]
}