{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "taskB_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UOxqzUqBbxnM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import gensim\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\",force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxsfCaDJlYjH",
        "outputId": "a8798181-946b-406c-99b8-132cf53a006f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxws60SulZ8Z",
        "outputId": "3a63cedd-db5c-4a7b-8bf3-f91c3e6e5640"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "# df = pd.read_csv('twitterA_train_data.txt', sep=\"\\t\",names=['Number','Label','Text'])\n",
        "df = pd.read_csv('twitter-2016train-BD.txt', sep=\"\\t\",names=['Number','Topic', 'Label','Text'])\n",
        "# df = pd.read_csv('SemEval2017-task4-train.subtask-A.arabic.txt', sep=\"\\t\",names=['Number','Label','Text'])\n",
        "df[\"tidy\"] = df[\"Text\"]\n",
        "\n",
        "# split data\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "metadata": {
        "id": "8S5jHD-mRtLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31919d6-9582-4471-e4a8-51e78263bb29"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN size: 3447\n",
            "TEST size: 862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wotd2vec\n",
        "def word_to_vector(df):\n",
        "  doc_text = [_text.split() for _text in df.Text] \n",
        "  doc_topic = [_text.split() for _text in df.Topic] \n",
        "  docs = doc_text + doc_topic\n",
        "  # docs = [_text.split() for _text in df.Text] \n",
        "  w2v_model = gensim.models.word2vec.Word2Vec(size=300, window=7, min_count=10, workers=8)\n",
        "  w2v_model.build_vocab(docs)\n",
        "  words = w2v_model.wv.vocab.keys()\n",
        "  vocab_size = len(words)\n",
        "  w2v_model.train(docs, total_examples=len(docs), epochs=8)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(df.Text)\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  return tokenizer, vocab_size, w2v_model"
      ],
      "metadata": {
        "id": "4Nz9-tIkGJjS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare train test data\n",
        "def data_prepare(tokenizer):\n",
        "  x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.tidy), maxlen=300)\n",
        "  x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.tidy), maxlen=300)\n",
        "  labels = df_train.Label.unique().tolist()\n",
        "  print(x_train)\n",
        "  print(x_test)\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(df_train.Label.tolist())\n",
        "\n",
        "  y_train = encoder.transform(df_train.Label.tolist())\n",
        "  y_test = encoder.transform(df_test.Label.tolist())\n",
        "\n",
        "  y_train = y_train.reshape(-1,1)\n",
        "  y_test = y_test.reshape(-1,1)\n",
        "  print(y_train.shape)\n",
        "  print(x_train.shape)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "def embedding_layer(vocab_size, w2v_model):\n",
        "  # embedding layer\n",
        "  # embedding_matrix = np.zeros((vocab_size, 300))\n",
        "  # print(embedding_matrix.shape)\n",
        "  # for word, i in tokenizer.word_index.items():\n",
        "  #   if word in w2v_model.wv:\n",
        "  #     embedding_matrix[i] = w2v_model.wv[word]\n",
        "  # print(embedding_matrix.shape)\n",
        "\n",
        "  # embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)\n",
        "  embedding_layer = Embedding(vocab_size, 300, input_length=300)\n",
        "\n",
        "  return embedding_layer\n",
        "\n",
        "def build_lstm_model(embedding_layer):\n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  # model.add(embedding_layer)\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(LSTM(1024, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_model(model, x_train, x_test, y_train, y_test):\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])\n",
        "  callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n",
        "  \n",
        "  y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
        "  y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
        "  print(y_train.shape)\n",
        "  print(x_train.shape)\n",
        "\n",
        "  y_train = np.array(y_train)\n",
        "  X_train = np.array(x_train)\n",
        "  y_test = np.array(y_test)\n",
        "  X_test = np.array(x_test)\n",
        "  print(y_train.shape)\n",
        "  print(x_train.shape)\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "                      batch_size=128,\n",
        "                      epochs=10,\n",
        "                      validation_split=0.1,\n",
        "                      verbose=1,\n",
        "                      callbacks=callbacks)\n",
        "  "
      ],
      "metadata": {
        "id": "vnRc9RpaFmnG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "tokenizer, vocab_size, w2v_model = word_to_vector(df)\n",
        "x_train, x_test, y_train, y_test = data_prepare(tokenizer)\n",
        "embedding_layer = embedding_layer(vocab_size, w2v_model)\n",
        "model = build_lstm_model(embedding_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwHagUSyF-pu",
        "outputId": "5eb3e318-078e-44d1-aff6-17007cf8f00b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0     0     0 ...    62    40    56]\n",
            " [    0     0     0 ...   373    10    14]\n",
            " [    0     0     0 ...    89   299  4988]\n",
            " ...\n",
            " [    0     0     0 ... 10386   728  1748]\n",
            " [    0     0     0 ...     3     4 11673]\n",
            " [    0     0     0 ...     3     4  5967]]\n",
            "[[    0     0     0 ...     3     4  4528]\n",
            " [    0     0     0 ...  2759  2760  3985]\n",
            " [    0     0     0 ...  2133    19   323]\n",
            " ...\n",
            " [    0     0     0 ...     3     4 10792]\n",
            " [    0     0     0 ...    17    29    19]\n",
            " [    0     0     0 ...    11   435  2412]]\n",
            "(3447, 1)\n",
            "(3447, 300)\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 300, 300)          3812100   \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 300, 300)          0         \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 256)               570368    \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1024)              263168    \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 3)                 3075      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,648,711\n",
            "Trainable params: 4,648,711\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train lstm model\n",
        "train_model(model, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LP5p1RnfxX7",
        "outputId": "ccc96816-c80a-4f5c-8f39-7718061c7589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3447, 3)\n",
            "(3447, 300)\n",
            "(3447, 3)\n",
            "(3447, 300)\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.9703WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
            "25/25 [==============================] - 170s 7s/step - loss: 0.1463 - accuracy: 0.9703 - val_loss: 0.5975 - val_accuracy: 0.7739 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9968WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
            "25/25 [==============================] - 167s 7s/step - loss: 0.0154 - accuracy: 0.9968 - val_loss: 0.6681 - val_accuracy: 0.7942 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
            "25/25 [==============================] - 166s 7s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8215 - val_accuracy: 0.8232 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "12/25 [=============>................] - ETA: 1:32 - loss: 0.0010 - accuracy: 1.0000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovycWMjjLud6",
        "outputId": "a4772aea-cd3b-4559-d979-6d978990d643"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final test accuracy\n",
        "score = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkSbIeQZiCkb",
        "outputId": "7c414c4c-c702-459c-b65d-9a5329aea727"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 9s 1s/step - loss: 0.4582 - accuracy: 0.8225\n",
            "\n",
            "ACCURACY: 0.8225057721138\n",
            "LOSS: 0.4581831395626068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train)\n",
        "x_train = np.array(x_train)\n",
        "y_test = np.array(y_test)\n",
        "x_test = np.array(x_test)\n",
        "print(y_train.shape)\n",
        "print(x_train.shape)\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
        "print(y_train.shape)\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CIEc8p9pbN1",
        "outputId": "1281f7fd-0fd5-42fb-e31d-c2e722af57b4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3447, 1)\n",
            "(3447, 300)\n",
            "(3447, 3)\n",
            "(3447, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cYvLonUmWBH3"
      }
    }
  ]
}